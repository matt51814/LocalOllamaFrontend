# Local Ollama

Must have ollama installed locally. Currently uses llama3.2:1b model. Will make this customizable going forward.

Start the application:
```docker compose build; docker compose up```

TODO:
1. Change chat to Markdown style and get the model to return responses in Markdown
2. Accept quotation marks without breaking everything
3. Add the ollama logo as the "avatar"
4. Have a chat side bar
5. support for other models 
