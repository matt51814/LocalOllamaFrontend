{
  "name": "local-ollama-frontend",
  "version": "1.0.0",
  "description": "Goal: aim to make a front end that works well with Ollama LLMs. I want to be able to download LLM models from Ollama and have a good looking front that emulate ChatGPT",
  "main": "local-ollama-frontend",
  "scripts": {
    "backend": "node src/ollama-express.js",
    "frontend": "live-server --port=5500",
    "test": "node --experimental-vm-modules node_modules/jest/bin/jest.js"
  },
  "devDependencies": {
    "cross-fetch": "^4.0.0",
    "jest": "^29.7.0",
    "jest-environment-jsdom": "^29.7.0",
    "live-server": "^1.2.2"
  },
  "author": "",
  "license": "ISC",
  "dependencies": {
    "cors": "^2.8.5",
    "express": "^4.21.1",
    "ollama": "^0.5.9"
  },
  "type": "module"
}
