{
  "name": "local-ollama-frontend",
  "version": "1.0.0",
  "description": "Goal: aim to make a front end that works well with Ollama LLMs. I want to be able to download LLM models from Ollama and have a good looking front that emulate ChatGPT",
  "main": "local-ollama-frontend",
  "scripts": {
    "backend": "node src/ollama-express.js",
    "frontend": "live-server --port=5500",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "devDependencies": {
    "live-server": "^1.2.2",
  },
  "author": "",
  "license": "ISC",
  "dependencies": {
    "cors": "^2.8.5",
    "express": "^4.21.1",
    "ollama": "^0.5.9"
  },
  "type": "module"
}
